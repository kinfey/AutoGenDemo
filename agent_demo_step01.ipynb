{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = autogen.config_list_from_json(\n",
    "    env_or_file=\"AOAI_CONFIG_LIST\",\n",
    "    file_location=\".\",\n",
    "    filter_dict={\n",
    "        \"model\": {\n",
    "            \"GPT3Model\",\n",
    "\n",
    "        }\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an AssistantAgent instance named \"assistant\"\n",
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config={\n",
    "        \"cache_seed\": 43,\n",
    "        \"config_list\": config_list,\n",
    "    }\n",
    ")\n",
    "# create a UserProxyAgent instance named \"user_proxy\"\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"ALWAYS\",\n",
    "    code_execution_config={\n",
    "        \"use_docker\": False\n",
    "    }, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "tell me today's top 10 news in the world from bbc news.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "To get today's top 10 news from the BBC News website, we can use web scraping to extract the information. We will need to install the `beautifulsoup4` library and `requests` library first.\n",
      "\n",
      "Here are the steps to follow:\n",
      "\n",
      "1. Install the required libraries:\n",
      "   `pip install beautifulsoup4 requests`\n",
      "\n",
      "2. Import the required libraries in your Python script:\n",
      "   ```python\n",
      "   from bs4 import BeautifulSoup\n",
      "   import requests\n",
      "   ```\n",
      "\n",
      "3. Send a GET request to the BBC News website and parse the HTML content:\n",
      "   ```python\n",
      "   url = 'https://www.bbc.co.uk/news'\n",
      "   response = requests.get(url)\n",
      "   soup = BeautifulSoup(response.content, 'html.parser')\n",
      "   ```\n",
      "\n",
      "4. Find the news headlines from the webpage:\n",
      "   ```python\n",
      "   news_headlines = soup.find_all('h3', class_='gs-c-promo-heading__title gel-pica-bold nw-o-link-split__text')\n",
      "   ```\n",
      "\n",
      "5. Print the top 10 news headlines:\n",
      "   ```python\n",
      "   for index, headline in enumerate(news_headlines[:10]):\n",
      "       print(f\"{index+1}. {headline.get_text()}\")\n",
      "   ```\n",
      "\n",
      "Here's the complete Python code:\n",
      "\n",
      "```python\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "\n",
      "url = 'https://www.bbc.co.uk/news'\n",
      "response = requests.get(url)\n",
      "soup = BeautifulSoup(response.content, 'html.parser')\n",
      "\n",
      "news_headlines = soup.find_all('h3', class_='gs-c-promo-heading__title gel-pica-bold nw-o-link-split__text')\n",
      "\n",
      "for index, headline in enumerate(news_headlines[:10]):\n",
      "    print(f\"{index+1}. {headline.get_text()}\")\n",
      "```\n",
      "\n",
      "Please note that web scraping may not be allowed on certain websites. Make sure to check the website's terms of service before scraping its content.\n",
      "\n",
      "Let me know if you need any further assistance!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "  File \"\", line 1\n",
      "    from bs4 import BeautifulSoup\n",
      "IndentationError: unexpected indent\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "I apologize for the error. It seems that there is an indentation issue in the code provided.\n",
      "\n",
      "Please make sure that the code is properly indented. The whole code block should be aligned to the left, without any additional indentation.\n",
      "\n",
      "Here's the corrected version of the code:\n",
      "\n",
      "```python\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "\n",
      "url = 'https://www.bbc.co.uk/news'\n",
      "response = requests.get(url)\n",
      "soup = BeautifulSoup(response.content, 'html.parser')\n",
      "\n",
      "news_headlines = soup.find_all('h3', class_='gs-c-promo-heading__title gel-pica-bold nw-o-link-split__text')\n",
      "\n",
      "for index, headline in enumerate(news_headlines[:10]):\n",
      "    print(f\"{index+1}. {headline.get_text()}\")\n",
      "```\n",
      "\n",
      "Please try running the code again with the corrected indentation. Let me know if it works or if you encounter any other issues.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "1. World breaches 1.5C warming threshold for full year\n",
      "2. North Koreans working in China 'exploited like slaves'\n",
      "3. Messi reaps Chinese fury after Japan friendly\n",
      "4. Why India’s law against cheating could be ineffective\n",
      "5. Indian player sparks conversation on sexism in chess\n",
      "6. S Korea leader breaks silence on Dior bag scandal\n",
      "7. Disney boss bets on Taylor Swift and Fortnite\n",
      "8. PNG hails 'big brother' Australia amid China debate\n",
      "9. Guarding the Middle East's most dangerous border\n",
      "10. Trapped orcas escape from drift ice near Japan\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "Great! The code executed successfully this time, and here are today's top 10 news headlines from BBC News:\n",
      "\n",
      "1. World breaches 1.5C warming threshold for full year\n",
      "2. North Koreans working in China 'exploited like slaves'\n",
      "3. Messi reaps Chinese fury after Japan friendly\n",
      "4. Why India’s law against cheating could be ineffective\n",
      "5. Indian player sparks conversation on sexism in chess\n",
      "6. S Korea leader breaks silence on Dior bag scandal\n",
      "7. Disney boss bets on Taylor Swift and Fortnite\n",
      "8. PNG hails 'big brother' Australia amid China debate\n",
      "9. Guarding the Middle East's most dangerous border\n",
      "10. Trapped orcas escape from drift ice near Japan\n",
      "\n",
      "If you have any more questions or need further assistance, feel free to ask!\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lokinfey/conda/envs/autogenenv/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:838: UserWarning: No summary_method provided or summary_method is not supported: \n",
      "  warnings.warn(\"No summary_method provided or summary_method is not supported: \")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_history=[{'content': \"tell me today's top 10 news in the world from bbc news.\", 'role': 'assistant'}, {'content': 'To get today\\'s top 10 news from the BBC News website, we can use web scraping to extract the information. We will need to install the `beautifulsoup4` library and `requests` library first.\\n\\nHere are the steps to follow:\\n\\n1. Install the required libraries:\\n   `pip install beautifulsoup4 requests`\\n\\n2. Import the required libraries in your Python script:\\n   ```python\\n   from bs4 import BeautifulSoup\\n   import requests\\n   ```\\n\\n3. Send a GET request to the BBC News website and parse the HTML content:\\n   ```python\\n   url = \\'https://www.bbc.co.uk/news\\'\\n   response = requests.get(url)\\n   soup = BeautifulSoup(response.content, \\'html.parser\\')\\n   ```\\n\\n4. Find the news headlines from the webpage:\\n   ```python\\n   news_headlines = soup.find_all(\\'h3\\', class_=\\'gs-c-promo-heading__title gel-pica-bold nw-o-link-split__text\\')\\n   ```\\n\\n5. Print the top 10 news headlines:\\n   ```python\\n   for index, headline in enumerate(news_headlines[:10]):\\n       print(f\"{index+1}. {headline.get_text()}\")\\n   ```\\n\\nHere\\'s the complete Python code:\\n\\n```python\\nfrom bs4 import BeautifulSoup\\nimport requests\\n\\nurl = \\'https://www.bbc.co.uk/news\\'\\nresponse = requests.get(url)\\nsoup = BeautifulSoup(response.content, \\'html.parser\\')\\n\\nnews_headlines = soup.find_all(\\'h3\\', class_=\\'gs-c-promo-heading__title gel-pica-bold nw-o-link-split__text\\')\\n\\nfor index, headline in enumerate(news_headlines[:10]):\\n    print(f\"{index+1}. {headline.get_text()}\")\\n```\\n\\nPlease note that web scraping may not be allowed on certain websites. Make sure to check the website\\'s terms of service before scraping its content.\\n\\nLet me know if you need any further assistance!', 'role': 'user'}, {'content': 'exitcode: 1 (execution failed)\\nCode output: \\n  File \"\", line 1\\n    from bs4 import BeautifulSoup\\nIndentationError: unexpected indent\\n', 'role': 'assistant'}, {'content': 'I apologize for the error. It seems that there is an indentation issue in the code provided.\\n\\nPlease make sure that the code is properly indented. The whole code block should be aligned to the left, without any additional indentation.\\n\\nHere\\'s the corrected version of the code:\\n\\n```python\\nfrom bs4 import BeautifulSoup\\nimport requests\\n\\nurl = \\'https://www.bbc.co.uk/news\\'\\nresponse = requests.get(url)\\nsoup = BeautifulSoup(response.content, \\'html.parser\\')\\n\\nnews_headlines = soup.find_all(\\'h3\\', class_=\\'gs-c-promo-heading__title gel-pica-bold nw-o-link-split__text\\')\\n\\nfor index, headline in enumerate(news_headlines[:10]):\\n    print(f\"{index+1}. {headline.get_text()}\")\\n```\\n\\nPlease try running the code again with the corrected indentation. Let me know if it works or if you encounter any other issues.', 'role': 'user'}, {'content': \"exitcode: 0 (execution succeeded)\\nCode output: \\n1. World breaches 1.5C warming threshold for full year\\n2. North Koreans working in China 'exploited like slaves'\\n3. Messi reaps Chinese fury after Japan friendly\\n4. Why India’s law against cheating could be ineffective\\n5. Indian player sparks conversation on sexism in chess\\n6. S Korea leader breaks silence on Dior bag scandal\\n7. Disney boss bets on Taylor Swift and Fortnite\\n8. PNG hails 'big brother' Australia amid China debate\\n9. Guarding the Middle East's most dangerous border\\n10. Trapped orcas escape from drift ice near Japan\\n\", 'role': 'assistant'}, {'content': \"Great! The code executed successfully this time, and here are today's top 10 news headlines from BBC News:\\n\\n1. World breaches 1.5C warming threshold for full year\\n2. North Koreans working in China 'exploited like slaves'\\n3. Messi reaps Chinese fury after Japan friendly\\n4. Why India’s law against cheating could be ineffective\\n5. Indian player sparks conversation on sexism in chess\\n6. S Korea leader breaks silence on Dior bag scandal\\n7. Disney boss bets on Taylor Swift and Fortnite\\n8. PNG hails 'big brother' Australia amid China debate\\n9. Guarding the Middle East's most dangerous border\\n10. Trapped orcas escape from drift ice near Japan\\n\\nIf you have any more questions or need further assistance, feel free to ask!\", 'role': 'user'}], summary='', cost=({'total_cost': 0.010852, 'gpt-35-turbo-16k': {'cost': 0.010852, 'prompt_tokens': 2644, 'completion_tokens': 730, 'total_tokens': 3374}}, {'total_cost': 0.010852, 'gpt-35-turbo-16k': {'cost': 0.010852, 'prompt_tokens': 2644, 'completion_tokens': 730, 'total_tokens': 3374}}), human_input=['', '', 'exit'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = \"tell me today's top 10 news in the world from bbc news.\"\n",
    "\n",
    "# the assistant receives a message from the user, which contains the task description\n",
    "user_proxy.initiate_chat(assistant, message=messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogenenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
